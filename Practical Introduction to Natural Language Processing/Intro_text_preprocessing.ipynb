{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZY2NYnvgo4_"
      },
      "source": [
        "# NLTK"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlFVOZfTgo5D"
      },
      "source": [
        "#### Install NLTK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mLQx904Fgo5E",
        "outputId": "49603877-a406-4551-f9c4-b1dd92dc164d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.9/dist-packages (3.8.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.9/dist-packages (from nltk) (2022.10.31)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk) (1.1.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from nltk) (4.65.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CciCitngo5G"
      },
      "source": [
        "#### Download models or corpora"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0_cVKKxgo5H",
        "outputId": "794473c6-956e-4f12-b520-9725f26da6a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/lib/python3.9/runpy.py:127: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> q'\n",
            "Command \"q'\" unrecognized\n",
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> q\n"
          ]
        }
      ],
      "source": [
        "!python -m nltk.downloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IS44955ngo5H"
      },
      "source": [
        "#### Import and use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZLkk0M5fgo5I",
        "outputId": "6f0189f0-6b41-4e74-aada-a46f1cc7aa81",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6NsrTsV9go5J"
      },
      "outputs": [],
      "source": [
        "tweet = \"@elonmusk digital currency is the future! :) #dogecoin\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5820w6M1go5K"
      },
      "outputs": [],
      "source": [
        "query = 'future'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fm0uSkRYgo5K",
        "outputId": "7e650150-6e16-41bf-e20e-91c1c8f952dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "34"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "tweet.find(query) # Not great solution for searching for a term... (What if I looked for \"dig\"?)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IeERi57ggo5L"
      },
      "source": [
        "#### Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p0QRcjk-go5L",
        "outputId": "5803268b-6971-4689-fb84-b2e99d3a1092",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['@elonmusk', 'digital', 'currency', 'is', 'the', 'future!', ':)', '#dogecoin']"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ],
      "source": [
        "tweet.split() # Also not great..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SfXWnbsTgo5M",
        "outputId": "09dce584-0719-40b1-d380-d251bb573fb9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[False]"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "[\"future\" in tweet.split()] # Also not great..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dYPgmMRDgo5M",
        "outputId": "98436d67-732d-4b66-df79-f5fddc5bec8e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['@',\n",
              " 'elonmusk',\n",
              " 'digital',\n",
              " 'currency',\n",
              " 'is',\n",
              " 'the',\n",
              " 'future',\n",
              " '!',\n",
              " ':',\n",
              " ')',\n",
              " '#',\n",
              " 'dogecoin']"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ],
      "source": [
        "nltk.word_tokenize(tweet) # Bingo!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gu305xZVgo5N",
        "outputId": "96efa3f4-0d38-4a36-9e74-64e0d8b75809",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[True]"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ],
      "source": [
        "[query in nltk.word_tokenize(tweet)] # Bingo!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### More options in customizing your tokenizer"
      ],
      "metadata": {
        "id": "ZZ035J28J77K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bICONsgugo5N",
        "outputId": "82107df9-8b8c-4748-90d3-c52ea7a6842b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['@',\n",
              " 'elonmusk',\n",
              " 'digital',\n",
              " 'currency',\n",
              " 'is',\n",
              " 'the',\n",
              " 'future',\n",
              " '!',\n",
              " ':',\n",
              " ')',\n",
              " '#',\n",
              " 'dogecoin']"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ],
      "source": [
        "nltk.word_tokenize(tweet, language='spanish')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vwOIm8Ppgo5O"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "custom_tokenizer = RegexpTokenizer('[a-zA-Z0-9]+', discard_empty=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OOS9SBtdgo5O",
        "outputId": "514ee9b2-a503-42a6-8b75-5cd0c4972501",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['elonmusk', 'digital', 'currency', 'is', 'the', 'future', 'dogecoin']"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ],
      "source": [
        "custom_tokenizer.tokenize(tweet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jS4Ujswqgo5P"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "tweet_tokenizer = TweetTokenizer(strip_handles=True, reduce_len=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "odSRKHsXgo5P",
        "outputId": "eecbbb44-0208-474e-b784-9a01614a04a0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['digital', 'currency', 'is', 'the', 'future', '!', ':)', '#dogecoin']"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ],
      "source": [
        "tweet_tokenizer.tokenize(tweet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2pr5Invxgo5P",
        "outputId": "e8a60181-d151-48fc-c311-2d05fc5de45a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['digital_currency', 'is', 'the', 'future', '!', ':)', '#dogecoin']"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ],
      "source": [
        "from nltk.tokenize import MWETokenizer\n",
        "mwe = MWETokenizer()\n",
        "mwe.add_mwe(('digital', 'currency'))\n",
        "mwe.tokenize(tweet_tokenizer.tokenize(tweet))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KPvhO_k0go5Q",
        "outputId": "07600473-30c0-4db8-8009-4106f0d5469c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ],
      "source": [
        "query = 'digital'\n",
        "query in mwe.tokenize(tweet_tokenizer.tokenize(tweet))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_r212dFcgo5R"
      },
      "source": [
        "### Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LaZbZf3tgo5R",
        "outputId": "d2d14fa4-4d28-4842-eb17-981f19ed1da7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'@elonmusk digital currency is the future! :) #dogecoin'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 62
        }
      ],
      "source": [
        "tweet.lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4REb4QpCgo5R"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import string\n",
        "\n",
        "def normalize_tokens(tokenized_text):\n",
        "    # Lowercase\n",
        "    tokens = [t.lower() for t in tokenized_text]\n",
        "    # Remove hashtags\n",
        "    tokens = [t for t in tokens if not t.startswith('#')]\n",
        "    # Remove punctuation\n",
        "    tokens = [t for t in tokens if t not in string.punctuation]\n",
        "    # Keep only letters\n",
        "#     tokens = [t for t in tokens if re.match('^[a-z]+$', t)]\n",
        "    # Normalize characters\n",
        "#     tokens = [re.sub('á', 'a', t) for t in tokens]\n",
        "    tokens = [re.sub('ă', 'a', t) for t in tokens]\n",
        "\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kt-MiwD5go5S",
        "outputId": "cbb7f1a1-ad3f-4201-931a-26748deb92dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['moneda', 'digitala']"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ],
      "source": [
        "romanian_query = 'monedă digitală'\n",
        "normalize_tokens(tweet_tokenizer.tokenize(romanian_query))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unidecode"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YNKJX0WP4a0z",
        "outputId": "cbe85d70-8411-4cec-ec58-daaceeea4c6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.9/dist-packages (1.3.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bpQehX46go5S",
        "outputId": "dd79e13b-cea8-47fa-ae0a-cd259f0990cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'moneda digitala'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 75
        }
      ],
      "source": [
        "import unidecode\n",
        "unidecode.unidecode(romanian_query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "22vT0ccIgo5S",
        "outputId": "c2e92fdf-87a6-45ed-c914-eb74d0092607",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['digital', 'currency', 'is', 'the', 'future', ':)']\n"
          ]
        }
      ],
      "source": [
        "normalized_tweet = normalize_tokens(tweet_tokenizer.tokenize(tweet))\n",
        "print(normalized_tweet)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MksTMBo9go5U"
      },
      "source": [
        "#### Stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JkfEuEcIgo5U",
        "outputId": "de3911b6-aa18-45c4-c647-44a3cd7e37e1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ],
      "source": [
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KVI5F1shgo5U",
        "outputId": "57471e8f-4808-4a80-873e-65c3a9a7831c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['a',\n",
              " 'abia',\n",
              " 'acea',\n",
              " 'aceasta',\n",
              " 'această',\n",
              " 'aceea',\n",
              " 'aceeasi',\n",
              " 'acei',\n",
              " 'aceia',\n",
              " 'acel',\n",
              " 'acela',\n",
              " 'acelasi',\n",
              " 'acele',\n",
              " 'acelea',\n",
              " 'acest',\n",
              " 'acesta',\n",
              " 'aceste',\n",
              " 'acestea',\n",
              " 'acestei',\n",
              " 'acestia',\n",
              " 'acestui',\n",
              " 'aceşti',\n",
              " 'aceştia',\n",
              " 'adica',\n",
              " 'ai',\n",
              " 'aia',\n",
              " 'aibă',\n",
              " 'aici',\n",
              " 'al',\n",
              " 'ala',\n",
              " 'ale',\n",
              " 'alea',\n",
              " 'alt',\n",
              " 'alta',\n",
              " 'altceva',\n",
              " 'altcineva',\n",
              " 'alte',\n",
              " 'altfel',\n",
              " 'alti',\n",
              " 'altii',\n",
              " 'altul',\n",
              " 'am',\n",
              " 'anume',\n",
              " 'apoi',\n",
              " 'ar',\n",
              " 'are',\n",
              " 'as',\n",
              " 'asa',\n",
              " 'asta',\n",
              " 'astea',\n",
              " 'astfel',\n",
              " 'asupra',\n",
              " 'atare',\n",
              " 'atat',\n",
              " 'atata',\n",
              " 'atatea',\n",
              " 'atatia',\n",
              " 'ati',\n",
              " 'atit',\n",
              " 'atita',\n",
              " 'atitea',\n",
              " 'atitia',\n",
              " 'atunci',\n",
              " 'au',\n",
              " 'avea',\n",
              " 'avem',\n",
              " 'aveţi',\n",
              " 'avut',\n",
              " 'aş',\n",
              " 'aţi',\n",
              " 'ba',\n",
              " 'ca',\n",
              " 'cam',\n",
              " 'cand',\n",
              " 'care',\n",
              " 'careia',\n",
              " 'carora',\n",
              " 'caruia',\n",
              " 'cat',\n",
              " 'catre',\n",
              " 'ce',\n",
              " 'cea',\n",
              " 'ceea',\n",
              " 'cei',\n",
              " 'ceilalti',\n",
              " 'cel',\n",
              " 'cele',\n",
              " 'celor',\n",
              " 'ceva',\n",
              " 'chiar',\n",
              " 'ci',\n",
              " 'cind',\n",
              " 'cine',\n",
              " 'cineva',\n",
              " 'cit',\n",
              " 'cita',\n",
              " 'cite',\n",
              " 'citeva',\n",
              " 'citi',\n",
              " 'citiva',\n",
              " 'cu',\n",
              " 'cui',\n",
              " 'cum',\n",
              " 'cumva',\n",
              " 'cât',\n",
              " 'câte',\n",
              " 'câtva',\n",
              " 'câţi',\n",
              " 'cînd',\n",
              " 'cît',\n",
              " 'cîte',\n",
              " 'cîtva',\n",
              " 'cîţi',\n",
              " 'că',\n",
              " 'căci',\n",
              " 'cărei',\n",
              " 'căror',\n",
              " 'cărui',\n",
              " 'către',\n",
              " 'da',\n",
              " 'daca',\n",
              " 'dacă',\n",
              " 'dar',\n",
              " 'dat',\n",
              " 'dată',\n",
              " 'dau',\n",
              " 'de',\n",
              " 'deasupra',\n",
              " 'deci',\n",
              " 'decit',\n",
              " 'deja',\n",
              " 'desi',\n",
              " 'despre',\n",
              " 'deşi',\n",
              " 'din',\n",
              " 'dintr',\n",
              " 'dintr-',\n",
              " 'dintre',\n",
              " 'doar',\n",
              " 'doi',\n",
              " 'doilea',\n",
              " 'două',\n",
              " 'drept',\n",
              " 'dupa',\n",
              " 'după',\n",
              " 'dă',\n",
              " 'e',\n",
              " 'ea',\n",
              " 'ei',\n",
              " 'el',\n",
              " 'ele',\n",
              " 'era',\n",
              " 'eram',\n",
              " 'este',\n",
              " 'eu',\n",
              " 'eşti',\n",
              " 'face',\n",
              " 'fara',\n",
              " 'fata',\n",
              " 'fel',\n",
              " 'fi',\n",
              " 'fie',\n",
              " 'fiecare',\n",
              " 'fii',\n",
              " 'fim',\n",
              " 'fiu',\n",
              " 'fiţi',\n",
              " 'foarte',\n",
              " 'fost',\n",
              " 'fără',\n",
              " 'i',\n",
              " 'ia',\n",
              " 'iar',\n",
              " 'ii',\n",
              " 'il',\n",
              " 'imi',\n",
              " 'in',\n",
              " 'inainte',\n",
              " 'inapoi',\n",
              " 'inca',\n",
              " 'incit',\n",
              " 'insa',\n",
              " 'intr',\n",
              " 'intre',\n",
              " 'isi',\n",
              " 'iti',\n",
              " 'la',\n",
              " 'le',\n",
              " 'li',\n",
              " 'lor',\n",
              " 'lui',\n",
              " 'lângă',\n",
              " 'lîngă',\n",
              " 'm',\n",
              " 'ma',\n",
              " 'mai',\n",
              " 'mea',\n",
              " 'mei',\n",
              " 'mele',\n",
              " 'mereu',\n",
              " 'meu',\n",
              " 'mi',\n",
              " 'mie',\n",
              " 'mine',\n",
              " 'mod',\n",
              " 'mult',\n",
              " 'multa',\n",
              " 'multe',\n",
              " 'multi',\n",
              " 'multă',\n",
              " 'mulţi',\n",
              " 'mâine',\n",
              " 'mîine',\n",
              " 'mă',\n",
              " 'ne',\n",
              " 'ni',\n",
              " 'nici',\n",
              " 'nimeni',\n",
              " 'nimic',\n",
              " 'niste',\n",
              " 'nişte',\n",
              " 'noastre',\n",
              " 'noastră',\n",
              " 'noi',\n",
              " 'nostri',\n",
              " 'nostru',\n",
              " 'nou',\n",
              " 'noua',\n",
              " 'nouă',\n",
              " 'noştri',\n",
              " 'nu',\n",
              " 'numai',\n",
              " 'o',\n",
              " 'or',\n",
              " 'ori',\n",
              " 'oricare',\n",
              " 'orice',\n",
              " 'oricine',\n",
              " 'oricum',\n",
              " 'oricând',\n",
              " 'oricât',\n",
              " 'oricînd',\n",
              " 'oricît',\n",
              " 'oriunde',\n",
              " 'pai',\n",
              " 'parca',\n",
              " 'patra',\n",
              " 'patru',\n",
              " 'pe',\n",
              " 'pentru',\n",
              " 'peste',\n",
              " 'pic',\n",
              " 'pina',\n",
              " 'poate',\n",
              " 'pot',\n",
              " 'prea',\n",
              " 'prima',\n",
              " 'primul',\n",
              " 'prin',\n",
              " 'printr-',\n",
              " 'putini',\n",
              " 'puţin',\n",
              " 'puţina',\n",
              " 'puţină',\n",
              " 'până',\n",
              " 'pînă',\n",
              " 'sa',\n",
              " 'sa-mi',\n",
              " 'sa-ti',\n",
              " 'sai',\n",
              " 'sale',\n",
              " 'sau',\n",
              " 'se',\n",
              " 'si',\n",
              " 'sint',\n",
              " 'sintem',\n",
              " 'spate',\n",
              " 'spre',\n",
              " 'sub',\n",
              " 'sunt',\n",
              " 'suntem',\n",
              " 'sunteţi',\n",
              " 'sus',\n",
              " 'să',\n",
              " 'săi',\n",
              " 'său',\n",
              " 't',\n",
              " 'ta',\n",
              " 'tale',\n",
              " 'te',\n",
              " 'ti',\n",
              " 'tine',\n",
              " 'toata',\n",
              " 'toate',\n",
              " 'toată',\n",
              " 'tocmai',\n",
              " 'tot',\n",
              " 'toti',\n",
              " 'totul',\n",
              " 'totusi',\n",
              " 'totuşi',\n",
              " 'toţi',\n",
              " 'trei',\n",
              " 'treia',\n",
              " 'treilea',\n",
              " 'tu',\n",
              " 'tuturor',\n",
              " 'tăi',\n",
              " 'tău',\n",
              " 'u',\n",
              " 'ul',\n",
              " 'ului',\n",
              " 'un',\n",
              " 'una',\n",
              " 'unde',\n",
              " 'undeva',\n",
              " 'unei',\n",
              " 'uneia',\n",
              " 'unele',\n",
              " 'uneori',\n",
              " 'unii',\n",
              " 'unor',\n",
              " 'unora',\n",
              " 'unu',\n",
              " 'unui',\n",
              " 'unuia',\n",
              " 'unul',\n",
              " 'v',\n",
              " 'va',\n",
              " 'vi',\n",
              " 'voastre',\n",
              " 'voastră',\n",
              " 'voi',\n",
              " 'vom',\n",
              " 'vor',\n",
              " 'vostru',\n",
              " 'vouă',\n",
              " 'voştri',\n",
              " 'vreo',\n",
              " 'vreun',\n",
              " 'vă',\n",
              " 'zi',\n",
              " 'zice',\n",
              " 'îi',\n",
              " 'îl',\n",
              " 'îmi',\n",
              " 'în',\n",
              " 'îţi',\n",
              " 'ăla',\n",
              " 'ălea',\n",
              " 'ăsta',\n",
              " 'ăstea',\n",
              " 'ăştia',\n",
              " 'şi',\n",
              " 'ţi',\n",
              " 'ţie']"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ],
      "source": [
        "from nltk.corpus import stopwords\n",
        "stopwords.words('romanian')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rH-ekGL3go5V",
        "outputId": "6a50acd2-8489-4089-9854-484ea510df5c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['digital', 'currency', 'future', ':)']\n"
          ]
        }
      ],
      "source": [
        "cleaned_tweet = [t for t in normalized_tweet if t not in stopwords.words(\"english\")]\n",
        "print(cleaned_tweet)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_L9GWe87go5V"
      },
      "source": [
        "#### Stemming / Lemmatization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DwKmEiS9go5V"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.stem import WordNetLemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# WordNet and OpenMultilingualWordnet necessary for lemmatization\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmrctMmM4vxn",
        "outputId": "7f157e9f-eb47-46f7-b7c8-8261d41f76b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PMrjEi_wgo5V",
        "outputId": "5d8a7076-29da-4e9d-effc-0f1b15c83770",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['digit', 'currenc', 'is', 'the', 'futur', ':)']"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ],
      "source": [
        "stemmer = PorterStemmer()\n",
        "\n",
        "[stemmer.stem(t) for t in normalized_tweet]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QGS64z4Sgo5W",
        "outputId": "051139a3-e923-4745-a6b2-c5950c32c375",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['digit', 'currenc', 'is', 'the', 'futur', ':)']"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ],
      "source": [
        "stemmer = SnowballStemmer(language='english') # This one has support for Romanian too!\n",
        "\n",
        "[stemmer.stem(t) for t in normalized_tweet]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bPJdL7efgo5W",
        "outputId": "4e00d504-cc30-4303-8a8a-939f91519362",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['digital', 'currency', 'is', 'the', 'future', ':)']"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ],
      "source": [
        "lemmatizer = WordNetLemmatizer() # Doesn't have support for Romanian ... see Spacy instead\n",
        "\n",
        "[lemmatizer.lemmatize(t) for t in normalized_tweet]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Combine with part-of-speech tagging!"
      ],
      "metadata": {
        "id": "KvXniP1HL-F6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Trained tagger needed for POS-tagging:\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "nltk.pos_tag(normalized_tweet)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSruFihJMFgP",
        "outputId": "3f037048-54c3-481b-a859-990bb17f9342"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('digital', 'JJ'),\n",
              " ('currency', 'NN'),\n",
              " ('is', 'VBZ'),\n",
              " ('the', 'DT'),\n",
              " ('future', 'NN'),\n",
              " (':)', 'NN')]"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B0QR4GB6go5W"
      },
      "outputs": [],
      "source": [
        "\n",
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "tag_map = {'J': wn.ADJ, 'V': wn.VERB, 'R': wn.ADV, 'N': wn.NOUN}\n",
        "def get_lemmas(tokenized_text):\n",
        "    tagged_text = nltk.pos_tag(tokenized_text)\n",
        "    return [lemmatizer.lemmatize(w, pos=tag_map.get(p[0], wn.NOUN)) for (w, p) in tagged_text]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b9Tftzy_go5X",
        "outputId": "7eea55fc-eea4-492b-8fe7-3be94e9a1385",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['digital', 'currencies']\n"
          ]
        }
      ],
      "source": [
        "query = \"digital currencies\"\n",
        "normalized_query = normalize_tokens(tweet_tokenizer.tokenize(query))\n",
        "print(normalized_query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cc7OAzvdgo5X",
        "outputId": "bf578567-a077-4e2b-c345-f45c1ba72684",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['digital', 'currency', 'be', 'the', 'future', ':)']\n",
            "['digital', 'currency']\n"
          ]
        }
      ],
      "source": [
        "lemmatized_tweet = get_lemmas(normalized_tweet)\n",
        "lemmatized_query = get_lemmas(normalized_query)\n",
        "print(lemmatized_tweet)\n",
        "print(lemmatized_query)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we can properly match the two!\n",
        "\n",
        "print(set(lemmatized_tweet).intersection(lemmatized_query))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7YArJ8KTMP2i",
        "outputId": "42482483-7486-4021-ec48-a029166a3da1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'currency', 'digital'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDv67B02go5Z"
      },
      "source": [
        "#### Sentence segmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ig3gIxugo5Z"
      },
      "outputs": [],
      "source": [
        "query = \"I am too fast. I am too furious.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mIUrh4YRgo5Z"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import sent_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CweNmFNkgo5Z",
        "outputId": "4f9abb0f-b8b7-4dbd-f6ba-aafbe7487ec6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I am too fast.', 'I am too furious.']"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ],
      "source": [
        "sent_tokenize(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rkq4yB_-go5Z",
        "outputId": "05a0a191-3b22-4e7f-e220-98f5e69b1070",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Soy muy rápido!', 'Estoy muy furioso!']"
            ]
          },
          "metadata": {},
          "execution_count": 115
        }
      ],
      "source": [
        "spanish_tokenizer = nltk.data.load('tokenizers/punkt/PY3/spanish.pickle')\n",
        "spanish_query = 'Soy muy rápido! Estoy muy furioso!'\n",
        "spanish_tokenizer.tokenize(spanish_query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "66jVuyn3go5a",
        "outputId": "1d7738b7-dda6-4c1b-faf6-4008015c0b21",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['J.K. Rowling is rich.', 'I am not as rich as J.K.']"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ],
      "source": [
        "sent_tokenize(\"J.K. Rowling is rich. I am not as rich as J.K.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fv-IkfQkgo5a"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import PunktSentenceTokenizer\n",
        "PunktSentenceTokenizer??"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Spacy"
      ],
      "metadata": {
        "id": "ve5mUJzJR-oU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "_xL5STWslT2o",
        "outputId": "4170edba-26f5-42a8-e86c-23306687fd02",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.8/dist-packages (3.4.4)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.21.6)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.0.11)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (21.3)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.10.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (4.64.1)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.4.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.0.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy) (6.3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.0.9)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.25.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (8.1.6)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->spacy) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy) (2.0.1)\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "pip install spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D67GVCFZlT2r",
        "outputId": "863c9717-9be0-4281-937a-1596eb943317",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n",
            "2023-01-30 17:12:09.837250: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-sm==3.4.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.1/en_core_web_sm-3.4.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.8/dist-packages (from en-core-web-sm==3.4.1) (3.4.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.11.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.9)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.21.6)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.8)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.64.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.7)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.4.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.25.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (57.4.0)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.1.6)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.11)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.3.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (6.3.0)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (21.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.10.4)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.4)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.4.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.24.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.10)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.0.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdLfKhDWlT2r"
      },
      "source": [
        "#### Define the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqHJXu31lT2s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00d51050-ed0e-4855-bd25-2689bdd9d48f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n"
          ]
        }
      ],
      "source": [
        "import spacy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "un2qlOwYlT2t"
      },
      "source": [
        "#### Process the texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yfp2IgP2lT2t"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EB_gh7rmlT2u"
      },
      "outputs": [],
      "source": [
        "tweet = \"@elonmusk digital currency is the future! :) #dogecoin\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0WELKYLUlT2v",
        "outputId": "63351c4c-7b50-487e-aaa0-698bba9dff15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "@elonmusk @elonmusk PROPN NNP nmod @xxxx False False\n",
            "digital digital ADJ JJ amod xxxx True False\n",
            "currency currency NOUN NN nsubj xxxx True False\n",
            "is be AUX VBZ ROOT xx True True\n",
            "the the DET DT det xxx True True\n",
            "future future NOUN NN attr xxxx True False\n",
            "! ! PUNCT . punct ! False False\n",
            ":) :) PUNCT NFP ROOT :) False False\n",
            "# # SYM $ dep # False False\n",
            "dogecoin dogecoin VERB VB ROOT xxxx True False\n"
          ]
        }
      ],
      "source": [
        "text = nlp(tweet)\n",
        "for token in text:\n",
        "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
        "            token.shape_, token.is_alpha, token.is_stop)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Romanian example"
      ],
      "metadata": {
        "id": "-4NmuzdirX5V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we have support for Romanian for all preprocessing tasks!"
      ],
      "metadata": {
        "id": "t7OpmvrkSPPT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download ro_core_news_sm   # Get Romanian model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mIecR9QEraLj",
        "outputId": "34daffff-2c1e-4555-c4a4-b41f17b8debb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n",
            "2023-03-19 19:12:49.839927: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2023-03-19 19:12:49.840022: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2023-03-19 19:12:49.840037: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "2023-03-19 19:12:51.801870: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ro-core-news-sm==3.5.0\n",
            "  Using cached https://github.com/explosion/spacy-models/releases/download/ro_core_news_sm-3.5.0/ro_core_news_sm-3.5.0-py3-none-any.whl (12.9 MB)\n",
            "Requirement already satisfied: spacy<3.6.0,>=3.5.0 in /usr/local/lib/python3.9/dist-packages (from ro-core-news-sm==3.5.0) (3.5.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->ro-core-news-sm==3.5.0) (63.4.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->ro-core-news-sm==3.5.0) (3.0.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->ro-core-news-sm==3.5.0) (1.1.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->ro-core-news-sm==3.5.0) (3.3.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->ro-core-news-sm==3.5.0) (23.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->ro-core-news-sm==3.5.0) (4.65.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->ro-core-news-sm==3.5.0) (3.0.8)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->ro-core-news-sm==3.5.0) (1.22.4)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->ro-core-news-sm==3.5.0) (0.10.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->ro-core-news-sm==3.5.0) (2.4.6)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->ro-core-news-sm==3.5.0) (2.0.7)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->ro-core-news-sm==3.5.0) (2.27.1)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->ro-core-news-sm==3.5.0) (1.0.4)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->ro-core-news-sm==3.5.0) (2.0.8)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->ro-core-news-sm==3.5.0) (6.3.0)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->ro-core-news-sm==3.5.0) (8.1.9)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->ro-core-news-sm==3.5.0) (0.7.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->ro-core-news-sm==3.5.0) (1.0.9)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->ro-core-news-sm==3.5.0) (1.10.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->ro-core-news-sm==3.5.0) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.9/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->ro-core-news-sm==3.5.0) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->ro-core-news-sm==3.5.0) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->ro-core-news-sm==3.5.0) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->ro-core-news-sm==3.5.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->ro-core-news-sm==3.5.0) (3.4)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.9/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->ro-core-news-sm==3.5.0) (0.0.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.9/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->ro-core-news-sm==3.5.0) (0.7.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.9/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->ro-core-news-sm==3.5.0) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->spacy<3.6.0,>=3.5.0->ro-core-news-sm==3.5.0) (2.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('ro_core_news_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.lang.ro.examples import sentences \n",
        "\n",
        "nlp = spacy.load(\"ro_core_news_sm\")\n",
        "doc = nlp(sentences[0])\n",
        "print(doc.text)\n",
        "for token in doc:\n",
        "    print(token.text, token.lemma_, token.pos_, token.dep_)"
      ],
      "metadata": {
        "id": "IC7QMc918X9e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19d1a160-fd00-4508-9bbf-d7a888878dca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Apple plănuiește să cumpere o companie britanică pentru un miliard de dolari\n",
            "Apple Apple PROPN nsubj\n",
            "plănuiește plănuii AUX ROOT\n",
            "să să PART mark\n",
            "cumpere cumpăra AUX ccomp\n",
            "o un DET det\n",
            "companie companie NOUN obj\n",
            "britanică britanic ADJ amod\n",
            "pentru pentru ADP case\n",
            "un un DET det\n",
            "miliard miliard NUM obl\n",
            "de de ADP case\n",
            "dolari dolar NOUN nmod\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVOloB3Wgo5a"
      },
      "source": [
        "# Numericalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3nHDPpHofXy"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "import string"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrPdYR_T0cG_"
      },
      "source": [
        "# Very customizable:\n",
        "CountVectorizer??"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROZ8MWjVovs5"
      },
      "source": [
        "corpus = [\n",
        " 'This is the first document.',\n",
        " 'This document is the second document.',\n",
        " 'And this is the third one.',\n",
        " 'Is this the first document?',\n",
        " '@user This one is a tweet #meta ;)' \n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eR7G_FJToyTC"
      },
      "source": [
        "vectorizer = CountVectorizer()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oEfjc5bUo0EW",
        "outputId": "a1a15a2c-7a8c-4bd9-e34d-b5419b7d2c28"
      },
      "source": [
        "\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "print(vectorizer.get_feature_names_out())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['and' 'document' 'first' 'is' 'meta' 'one' 'second' 'the' 'third' 'this'\n",
            " 'tweet' 'user']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Va8nelMio2rB",
        "outputId": "7b6f37f7-742c-42e4-cd5b-cf167a00c50a"
      },
      "source": [
        "print(X.toarray())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 1 1 1 0 0 0 1 0 1 0 0]\n",
            " [0 2 0 1 0 0 1 1 0 1 0 0]\n",
            " [1 0 0 1 0 1 0 1 1 1 0 0]\n",
            " [0 1 1 1 0 0 0 1 0 1 0 0]\n",
            " [0 0 0 1 1 1 0 0 0 1 1 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Customizing the Vectorizer"
      ],
      "metadata": {
        "id": "eXtMBCkMhdRe"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2BccV7qTpG3M",
        "outputId": "fb7242d8-86e2-4f89-c515-840329ad9666"
      },
      "source": [
        "tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True)\n",
        "tokenize_funct = tokenizer.tokenize\n",
        "nltk.download('stopwords')\n",
        "word_blacklist = stopwords.words('english') + list(string.punctuation)\n",
        "vectorizer_tweet = CountVectorizer(tokenizer=tokenize_funct, stop_words=word_blacklist)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wInqXeK6pNqV",
        "outputId": "f1ea0706-1821-4ba6-c52a-7d7466f3c9d7"
      },
      "source": [
        "X1 = vectorizer_tweet.fit_transform(corpus)\n",
        "print(vectorizer_tweet.get_feature_names_out())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['#meta' ';)' 'document' 'first' 'one' 'second' 'third' 'tweet']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVagyaeqFYym",
        "outputId": "127e5a0f-2206-4188-a126-079bfe7db218"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<5x8 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 12 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {},
          "execution_count": 147
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer_tweet.transform([corpus[0]]).toarray()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCEskpHXFUYv",
        "outputId": "2ee4fe66-6f98-4d3a-891d-9b8b21dc4e15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 1, 1, 0, 0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizertfidf = TfidfVectorizer()\n",
        "Xtfidf = vectorizertfidf.fit_transform(corpus)\n",
        "print(vectorizertfidf.get_feature_names_out())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eCrXgOQAMdgx",
        "outputId": "d7f4352c-9df6-48a2-ccae-d34b0abb967c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['and' 'document' 'first' 'is' 'meta' 'one' 'second' 'the' 'third' 'this'\n",
            " 'tweet' 'user']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Xtfidf.toarray()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KjcO3WsSMtKj",
        "outputId": "39728d39-659c-40f9-988d-e6593eb30c50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.48961805, 0.58983706, 0.34836727, 0.        ,\n",
              "        0.        , 0.        , 0.41188214, 0.        , 0.34836727,\n",
              "        0.        , 0.        ],\n",
              "       [0.        , 0.70933829, 0.        , 0.25235002, 0.        ,\n",
              "        0.        , 0.52958485, 0.29835887, 0.        , 0.25235002,\n",
              "        0.        , 0.        ],\n",
              "       [0.54054601, 0.        , 0.        , 0.25757307, 0.        ,\n",
              "        0.43610912, 0.        , 0.3045342 , 0.54054601, 0.25757307,\n",
              "        0.        , 0.        ],\n",
              "       [0.        , 0.48961805, 0.58983706, 0.34836727, 0.        ,\n",
              "        0.        , 0.        , 0.41188214, 0.        , 0.34836727,\n",
              "        0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.23518498, 0.49356209,\n",
              "        0.39820278, 0.        , 0.        , 0.        , 0.23518498,\n",
              "        0.49356209, 0.49356209]])"
            ]
          },
          "metadata": {},
          "execution_count": 143
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text is now vectorized and ready for machine learning!"
      ],
      "metadata": {
        "id": "92Nqy2XPkNxz"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}